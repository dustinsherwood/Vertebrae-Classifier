# -*- coding: utf-8 -*-
"""bone classifier 2d to 2d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OuPfi9k0UzpeDwszFqsm4NyfNER8JG1b
"""

import tensorflow as tf
from google.colab import drive
drive.mount('/content/drive')
print("Tensorflow version " + tf.__version__)

# try:
#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
# except ValueError:
#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# tpu_strategy = tf.distribute.TPUStrategy(tpu)

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

def preprocess_image(image, label):
    # convert the image to grayscale
    image = tf.image.rgb_to_grayscale(image)
    # normalize pixel values to the range [0, 1]
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

# Define the path to your data directory
data_dir = '/content/drive/MyDrive/bones/picture_data'



train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='training',
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical')

train_dataset = train_dataset.map(
    preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

val_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='validation',
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical')

val_dataset = val_dataset.map(
    preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

"""view some of the datset images"""

samples = train_dataset.take(1)
print(samples)

# Loop over the samples and display each image
for new_batch, label_batch in samples:
  for i in range(len(new_batch)-10):
      plt.imshow(new_batch[i].numpy().squeeze(), cmap='gray')
      predicted_class = tf.argmax(label_batch[i])
      plt.title(f"Predicted class: {predicted_class}")
      plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Define the model architecture
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,1)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(4, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_dataset,
    epochs=10,
    validation_data=val_dataset)

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(val_dataset)
print('Validation loss:', loss)
print('Validation accuracy:', accuracy)

epochs = 10
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""Advisable to train the model again, creating a 6 epoch version and 2 epoch version as well as the 10 epoch version."""

model.save('/content/drive/MyDrive/classificationmodel2d2to2dmk1.h5')

"""# Load Model1"""

from tensorflow.keras.models import load_model
model = load_model('/content/drive/MyDrive/classificationmodel2d2to2dmk1.h5')
model.summary()

"""# Test Model1 on image"""

from tensorflow.keras.preprocessing import image

# Assume 'model' is already defined and loaded with weights

# Load an image and resize it to the required input size for the model
img_path = '/content/drive/MyDrive/bones/new data/IMG_20230321_140551828.jpg'
img = image.load_img(img_path, target_size=(224, 224))

# Convert the image to grayscale
img = img.convert('L')

#img = tf.image.rgb_to_grayscale(img)
# normalize pixel values to the range [0, 1]
#img = tf.cast(img, tf.float32) / 255.0

# Convert the image to a numpy array
img_array = image.img_to_array(img)

# Expand the dimensions to create a batch of size 1
img_batch = np.expand_dims(img_array, axis=0)

# Normalize the pixel values to be between 0 and 1
img_batch = img_batch / 255.0

# Make a prediction using the model
prediction = model.predict(img_batch)

class_labels = ['Mammals', 'Fish', 'Birds', 'Reptiles']

# Get the predicted class label
class_index = np.argmax(prediction)
class_label = class_labels[class_index]

# Get the probability of the predicted class
class_prob = prediction[0][class_index]

# Display the predicted class label and probability
print("Predicted class: {}, Probability: {:.2f}".format(class_label, class_prob))

plt.imshow(img_array.astype('uint8'))
plt.show()

"""# Test Accuracy on labeled set, each class"""

test_data_dir = '/content/drive/MyDrive/bones/test bones'

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_data_dir,
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode='categorical')
class_labels = test_dataset.class_names
print("Class labels:", class_labels)

test_dataset = test_dataset.map(
    preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

count = 0
for batch in test_dataset:
    count += len(batch[0])  # Add the number of instances in the current batch

print(f"Size of test_dataset: {count}")

#check accuracy
# Evaluate your model on the test dataset
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f"Test loss: {test_loss}, Test accuracy: {test_accuracy}")

import numpy as np
from sklearn.metrics import confusion_matrix

# Extract true labels from the test dataset
y_true = np.concatenate([y.numpy() for _, y in test_dataset])

# Convert one-hot encoded labels to class indices
y_true = np.argmax(y_true, axis=1)

# Generate predictions for the test dataset
y_pred = model.predict(test_dataset)

# Convert probabilities to class indices
y_pred = np.argmax(y_pred, axis=1)

from collections import Counter

# Get the indices of the correct predictions
correct_indices = np.where(y_true == y_pred)[0]
incorrect_indices = np.where(y_true != y_pred)[0]

# Get the true labels for the correct predictions
correct_labels = y_true[correct_indices]
incorrect_labels = y_true[incorrect_indices]



# Count the correct predictions for each class
correct_predictions_by_class = Counter(correct_labels)
incorrect_predictions_by_class = Counter(incorrect_labels)

print("Number of successful predictions by class:", correct_predictions_by_class)
print("Number of failed predictions by class:", incorrect_predictions_by_class)

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred)

print("Confusion Matrix:")
print(cm)

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=class_labels,
            yticklabels=class_labels)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# Test on some unlabled images"""

data_dir = '/content/drive/MyDrive/bones/new data'

def preprocess_labelless_image(image):
    # convert the image to grayscale
    image = tf.image.rgb_to_grayscale(image)
    # normalize pixel values to the range [0, 1]
    image = tf.cast(image, tf.float32) / 255.0
    return image

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    class_names=None,
    validation_split=None,
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    label_mode=None)

test_dataset = test_dataset.map(
    preprocess_labelless_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)

"""Predict Multiple Classes"""

# Generate predictions for the new dataset
predictions = model.predict(test_dataset)

# Get the predicted class for each image
predicted_classes = tf.argmax(predictions, axis=1)

# Convert the predicted class index to a class label
class_labels = ['Birds', 'Fish', 'Mammals', 'Reptiles']
predicted_labels = [class_labels[i] for i in predicted_classes]

# Print the predicted labels
print(predicted_labels)

samples = train_dataset.take(1)

# Loop over the samples and display each image
for new_batch, label_batch in test_dataset:
    for i in range(len(new_batch)):
        plt.imshow(new_batch[i].numpy().astype('uint8'), cmap='gray')
        predicted_class = tf.argmax(label_batch[i])
        plt.title(f"Predicted class: {predicted_class}")
        plt.show()

"""Predict a Single Class"""

# Generate predictions for the new dataset
predictions = model.predict(test_dataset)

# Compute the average predicted probability for each class
average_probabilities = tf.reduce_mean(predictions, axis=0)

# Get the index of the class with the highest average probability
predicted_class_index = tf.argmax(average_probabilities)

# Get the predicted class label
predicted_class_label = class_labels[predicted_class_index]

# Print the predicted class label
print(predicted_class_label)